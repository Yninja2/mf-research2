{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bff53e",
   "metadata": {},
   "source": [
    "# Lesson 1: Advanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb98470-c136-471d-a63e-d50d8eb09c57",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e2859b-596e-40b3-867b-f4d6e91f74bc",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "#    input_files=[\"./eBook-How-to-Build-a-Career-in-AI.pdf\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7d0857-b9d1-4feb-8243-bfd2f4953acd",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "87 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: 76fc43f9-2cc0-418e-b8f0-73ab2efbfd70\n",
      "Text: äº‹å‹™é€£çµ¡  å¹³æˆï¼’ï¼•å¹´ï¼”æœˆ ï¼‘ï¼•æ—¥    å„éƒ½é“åºœçœŒè¡›ç”Ÿä¸»ç®¡éƒ¨ï¼ˆå±€ï¼‰è–¬å‹™ä¸»ç®¡èª² å¾¡ä¸­       åšç”ŸåŠ´åƒçœåŒ»è–¬é£Ÿå“å±€å¯©æŸ»ç®¡ç†èª²\n",
      "ç´°èƒãƒ»çµ„ç¹”åŠ å·¥åŒ»è–¬å“ç­‰ã®è£½é€ ã«é–¢é€£ã™ã‚‹ã‚‚ã®ã«ä¿‚ã‚‹åŸè–¬ç­‰ç™»éŒ²åŸç°¿ ç™»éŒ²ç”³è«‹æ›¸åŠã³ãã®ç”³è«‹æ›¸ã«æ·»ä»˜ã™ã¹ãè³‡æ–™ã®ä½œæˆè¦é ˜ã«é–¢ã™ã‚‹ Q&A ã«ã¤ã„ã¦\n",
      "ã€Œç´°èƒãƒ»çµ„ç¹”åŠ å·¥åŒ»è–¬å“ç­‰ã®è£½é€ ã«é–¢é€£ã™ã‚‹ã‚‚ã®ã«ä¿‚ã‚‹åŸè–¬ç­‰ç™»éŒ²åŸç°¿ã€ ç™» éŒ²ç”³è«‹æ›¸åŠã³ãã®ç”³è«‹æ›¸ã«æ·»ä»˜ã™ã¹ãè³‡æ–™ã®ä½œæˆè¦é ˜ã€\n",
      "ã«ã¤ã„ã¦ã¯ã€å¹³æˆï¼’ï¼• å¹´ï¼“æœˆï¼˜æ—¥ä»˜ã‘äº‹å‹™é€£çµ¡ã«ã‚ˆã‚Šç¤ºã—ãŸã¨ã“ã‚ã§ã™ãŒã€ä»Šèˆ¬ã€è³ªç–‘å¿œç­”é›†ã‚’åˆ¥æ·»ã®ã¨ãŠã‚Šã¾ã¨ã‚ã¾ã—ãŸã®ã§ã€è²´ç®¡å†…é–¢ä¿‚è€…ã«å¯¾ã—å‘¨çŸ¥\n",
      "æ–¹ã‚ˆã‚ã—ãå¾¡é…æ…®ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3123d3d",
   "metadata": {},
   "source": [
    "## Basic RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4abc806-64f5-46bb-8c9f-6469ecb18d20",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc2baff-5e8b-4733-9899-16f248777b23",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17550a39f73549a6be0f0a052082ed45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555d96e5cc7c4ed8a7954febd1fe3214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074328cbe63f43c6bb05a778dec3c70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acf9d490f2e47c29d22002ed0127346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7373d0a3d905453b80f2f9eac7b49c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a8c0f606444808806b708f6d4a76c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "index = VectorStoreIndex.from_documents([document],\n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae52a26c-7d0c-44df-8043-4c7f19f794b9",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0d5b6e-cc2e-4648-b28c-5fa25a97d175",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã¯ã€ç™»éŒ²äº‹é …ã®å¤‰æ›´ã‚„æ‰¿ç¶™ã«é–¢é€£ã™ã‚‹åŒ»è–¬å“ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆMaster Fileï¼‰ã®ç•¥ç§°ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"MFã¨ã¯ãªã‚“ã§ã™ã‹?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a1ac5",
   "metadata": {},
   "source": [
    "## Evaluation setup using TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ead7dc1-71b2-4001-918f-bf8d610fd3fd",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n"
     ]
    }
   ],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a278f8",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# You can try your own question:\n",
    "new_question = \"What is the right AI job for me?\"\n",
    "eval_questions.append(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771b303",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5204e8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c063c9c7-bf1e-4b24-9a22-d4281c0f954e",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ğŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03802825-6ce4-4563-aeec-d8d57e095ad1",
   "metadata": {},
   "source": [
    "For the classroom, we've written some of the code in helper functions inside a utils.py file.  \n",
    "- You can view the utils.py file in the file directory by clicking on the \"Jupyter\" logo at the top of the notebook.\n",
    "- In later lessons, you'll get to work directly with the code that's currently wrapped inside these helper functions, to give you more options to customize your RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f754bed-d16f-4c8d-a1a1-b36096272570",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_prebuilt_trulens_recorder\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(query_engine,\n",
    "                                             app_id=\"Direct Query Engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dbdfbcc-aac7-4805-9894-4fc016c66bf6",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    for question in eval_questions:\n",
    "        response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14f512b-601c-42d0-bfac-bf41d9c577e7",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da4a602-0d56-4bf8-9fa6-03ef0b7e254b",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance_calls</th>\n",
       "      <th>Context Relevance_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_19a60c2e4f59a43b2b47a2b9920bbc36</td>\n",
       "      <td>\"MF\\u3068\\u306f\\u306a\\u3093\\u3067\\u3059\\u304b\\...</td>\n",
       "      <td>\"MF\\u306f\\u300c\\u539f\\u85ac\\u7b49\\u767b\\u9332\\...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_19a60c2e4f59a43b2b4...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2024-06-20T22:41:07.178819\", \"...</td>\n",
       "      <td>2024-06-20T22:41:08.317577</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[{'args': {'prompt': 'MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',...</td>\n",
       "      <td>[{'args': {'prompt': 'MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',...</td>\n",
       "      <td>1</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.003122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_75405ab6fa18d38f3eb5a7b478bf8728</td>\n",
       "      <td>\"MF\\u306e\\u7533\\u8acb\\u66f8\\u985e\\u306f\\u306a\\...</td>\n",
       "      <td>\"MF\\u306e\\u7533\\u8acb\\u66f8\\u985e\\u306f\\u3001\\...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_75405ab6fa18d38f3eb...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2024-06-20T22:41:08.498149\", \"...</td>\n",
       "      <td>2024-06-20T22:41:10.207188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>[{'args': {'prompt': 'MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•...</td>\n",
       "      <td>[{'args': {'prompt': 'MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•...</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>0.002938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_aba8da3dc93433af963f7478c748c9d4</td>\n",
       "      <td>\"USP\\u306b\\u767b\\u9332\\u304c\\u308b\\u539f\\u85ac...</td>\n",
       "      <td>\"\\u539f\\u85ac\\u306e\\u6b8b\\u7559\\u6eb6\\u5a92\\u3...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_aba8da3dc93433af963...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2024-06-20T22:41:10.374231\", \"...</td>\n",
       "      <td>2024-06-20T22:41:14.453109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>[{'args': {'prompt': 'USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯...</td>\n",
       "      <td>[{'args': {'prompt': 'USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯...</td>\n",
       "      <td>4</td>\n",
       "      <td>2235</td>\n",
       "      <td>0.003453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Direct Query Engine</td>\n",
       "      <td>{\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_4055214a132a80ad4bd223060703a338</td>\n",
       "      <td>\"MF\\u306e\\u4ed5\\u7d44\\u307f\\u3092\\u6559\\u3048\\...</td>\n",
       "      <td>\"MF\\u767b\\u9332\\u8005\\u306f\\u3001\\u533b\\u85ac\\...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_4055214a132a80ad4bd...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2024-06-20T22:41:14.616523\", \"...</td>\n",
       "      <td>2024-06-20T22:41:18.176021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.003138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                app_id                                           app_json  \\\n",
       "0  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "1  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "2  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "3  Direct Query Engine  {\"app_id\": \"Direct Query Engine\", \"tags\": \"-\",...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_19a60c2e4f59a43b2b47a2b9920bbc36   \n",
       "1  record_hash_75405ab6fa18d38f3eb5a7b478bf8728   \n",
       "2  record_hash_aba8da3dc93433af963f7478c748c9d4   \n",
       "3  record_hash_4055214a132a80ad4bd223060703a338   \n",
       "\n",
       "                                               input  \\\n",
       "0  \"MF\\u3068\\u306f\\u306a\\u3093\\u3067\\u3059\\u304b\\...   \n",
       "1  \"MF\\u306e\\u7533\\u8acb\\u66f8\\u985e\\u306f\\u306a\\...   \n",
       "2  \"USP\\u306b\\u767b\\u9332\\u304c\\u308b\\u539f\\u85ac...   \n",
       "3  \"MF\\u306e\\u4ed5\\u7d44\\u307f\\u3092\\u6559\\u3048\\...   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"MF\\u306f\\u300c\\u539f\\u85ac\\u7b49\\u767b\\u9332\\...    -   \n",
       "1  \"MF\\u306e\\u7533\\u8acb\\u66f8\\u985e\\u306f\\u3001\\...    -   \n",
       "2  \"\\u539f\\u85ac\\u306e\\u6b8b\\u7559\\u6eb6\\u5a92\\u3...    -   \n",
       "3  \"MF\\u767b\\u9332\\u8005\\u306f\\u3001\\u533b\\u85ac\\...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_19a60c2e4f59a43b2b4...   \n",
       "1  {\"record_id\": \"record_hash_75405ab6fa18d38f3eb...   \n",
       "2  {\"record_id\": \"record_hash_aba8da3dc93433af963...   \n",
       "3  {\"record_id\": \"record_hash_4055214a132a80ad4bd...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "1  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "2  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "3  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-06-20T22:41:07.178819\", \"...   \n",
       "1  {\"start_time\": \"2024-06-20T22:41:08.498149\", \"...   \n",
       "2  {\"start_time\": \"2024-06-20T22:41:10.374231\", \"...   \n",
       "3  {\"start_time\": \"2024-06-20T22:41:14.616523\", \"...   \n",
       "\n",
       "                           ts  Answer Relevance  Context Relevance  \\\n",
       "0  2024-06-20T22:41:08.317577               1.0               0.45   \n",
       "1  2024-06-20T22:41:10.207188               1.0               0.50   \n",
       "2  2024-06-20T22:41:14.453109               1.0               1.00   \n",
       "3  2024-06-20T22:41:18.176021               NaN                NaN   \n",
       "\n",
       "                              Answer Relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',...   \n",
       "1  [{'args': {'prompt': 'MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•...   \n",
       "2  [{'args': {'prompt': 'USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯...   \n",
       "3                                                NaN   \n",
       "\n",
       "                             Context Relevance_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'prompt': 'MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',...        1          2074   \n",
       "1  [{'args': {'prompt': 'MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•...        1          1940   \n",
       "2  [{'args': {'prompt': 'USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯...        4          2235   \n",
       "3                                                NaN        3          2023   \n",
       "\n",
       "   total_cost  \n",
       "0    0.003122  \n",
       "1    0.002938  \n",
       "2    0.003453  \n",
       "3    0.003138  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64310897-179b-4081-aab8-f08a3392a078",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba4c6fa46da4bf09779b6baffce14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at https://s172-31-2-34p15913.lab-aws-production.deeplearning.ai/ .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eedcef",
   "metadata": {},
   "source": [
    "## Advanced RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17ea2b",
   "metadata": {},
   "source": [
    "### 1. Sentence Window retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dae4a668-3699-4750-82f7-e53ae1bca3a7",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78f7678f-358d-448d-b153-11ac8e96a7fc",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import build_sentence_window_index\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72f904c3-9845-4df5-9d2e-e5115160f987",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_sentence_window_query_engine\n",
    "\n",
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f59e2314-7cac-42f4-a552-9a8e4db641eb",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã¯ã€ŒåŸè–¬ç­‰ç™»éŒ²åŸç°¿ã€ã®ç•¥ç§°ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\"\n",
    ")\n",
    "print(str(window_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5c10917-8846-4e73-838d-6232c906a7db",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tru.reset_database()\n",
    "\n",
    "tru_recorder_sentence_window = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine,\n",
    "    app_id = \"Sentence Window Query Engine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11710e67-aba8-479e-8585-c4c611e2c1d2",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7fdd786ee0b0 is calling an instrumented method <function BaseQueryEngine.query at 0x7fdef3f2eef0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x7fdd884af610) using this function.\n",
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7fdd786ee0b0 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7fdeef34eb90>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x7fdd884af610) using this function.\n",
      "A new object of type <class 'llama_index.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> at 0x7fdd786e8910 is calling an instrumented method <function BaseRetriever.retrieve at 0x7fdef3f2e290>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.retriever based on other object (0x7fdd8847c520) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7fdd786ee140 is calling an instrumented method <function CompactAndRefine.get_response at 0x7fdef3969ea0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x7fdd884af550) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7fdd786ee140 is calling an instrumented method <function Refine.get_response at 0x7fdef2970ee0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x7fdd884af550) using this function.\n",
      "A new object of type <class 'llama_index.llm_predictor.base.LLMPredictor'> at 0x7fdd883cf640 is calling an instrumented method <function LLMPredictor.predict at 0x7fdeff63c0d0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer.service_context.llm_predictor based on other object (0x7fdda81b1ec0) using this function.\n",
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7fdd786ee0b0 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7fdeef34eb90>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x7fdd884af610) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã¯ã€ŒåŸè–¬ç­‰ç™»éŒ²åŸç°¿ã€ã®ç•¥ç§°ã§ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33963 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33692 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33963 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33692 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33692 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33963 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=1.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7fdd786ee140 is calling an instrumented method <function Refine.get_response at 0x7fdef2970ee0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x7fdd884af550) using this function.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33692 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 481, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 377, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 303, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 480, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 482, in <lambda>\n",
      "    lambda: self.imp(**ins)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py\", line 151, in groundedness_measure_with_cot_reasons\n",
      "    reason = self.groundedness_provider._groundedness_doc_in_out(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 110, in _groundedness_doc_in_out\n",
      "    return self.endpoint.run_me(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 215, in run_me\n",
      "    raise RuntimeError(\n",
      "RuntimeError: API openai request failed 4 time(s).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 486, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of groundedness_measure_with_cot_reasons failed on inputs: \n",
      "{'source': ' \\n'\n",
      "           'äº‹å‹™é€£çµ¡ \\n'\n",
      "           'å¹³æˆï¼’ï¼•å¹´ï¼”æœˆ ï¼‘ï¼•æ—¥ \\n'\n",
      "           ' \\n'\n",
      "           'å„éƒ½é“åºœçœŒè¡›ç”Ÿä¸»ç®¡éƒ¨ï¼ˆå±€ï¼‰è–¬å‹™ä¸»ç®¡èª² å¾¡ä¸­ \\n'\n",
      "       \n",
      "API openai request failed 4 time(s)..\n",
      "\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33963 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 481, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 377, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 303, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 480, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 482, in <lambda>\n",
      "    lambda: self.imp(**ins)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 334, in relevance_with_cot_reasons\n",
      "    return self._extract_score_and_reasons_from_response(system_prompt)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 142, in _extract_score_and_reasons_from_response\n",
      "    response = self.endpoint.run_me(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 215, in run_me\n",
      "    raise RuntimeError(\n",
      "RuntimeError: API openai request failed 4 time(s).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 486, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of relevance_with_cot_reasons failed on inputs: \n",
      "{'prompt': 'MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',\n",
      " 'response': ' \\n'\n",
      "             'äº‹å‹™é€£çµ¡ \\n'\n",
      "             'å¹³æˆï¼’ï¼•å¹´ï¼”æœˆ ï¼‘ï¼•æ—¥ \\n'\n",
      "             ' \\n'\n",
      "\n",
      "API openai request failed 4 time(s)..\n",
      "\n",
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7fdd786ee0b0 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7fdeef34eb90>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x7fdd884af610) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã®ç”³è«‹æ›¸é¡ã¯ã€åŒ»è–¬å“ã«é–¢ã™ã‚‹MFç™»éŒ²ã«ä½¿ç”¨ã™ã‚‹é›»å­çš„ãªæ§˜å¼ã«ã¤ã„ã¦ã®è¨˜è¼‰äº‹é …ã¨ã€ç™»éŒ²ç”³è«‹æ›¸ã«æ·»ä»˜ã™ã‚‹è³‡æ–™ï¼ˆç™»éŒ²ãƒ‡ãƒ¼ã‚¿ï¼‰ã«é–¢ã™ã‚‹æ§˜å¼ãŒã‚ã‚Šã¾ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7fdd786ee140 is calling an instrumented method <function Refine.get_response at 0x7fdef2970ee0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x7fdd884af550) using this function.\n",
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x7fdd786ee0b0 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x7fdeef34eb90>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x7fdd884af610) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯ã€è£½é€ æ–¹æ³•ã€è£½é€ å·¥ç¨‹ç®¡ç†ã€å“è³ªç®¡ç†è©¦é¨“ã€è¦æ ¼åŠã³è©¦é¨“æ–¹æ³•ã€å®‰å®šæ€§è©¦é¨“ã€éè‡¨åºŠè©¦é¨“ï¼ˆä¸»ã¨ã—ã¦æ–°æ·»åŠ å‰¤ï¼‰ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç™»éŒ²ã™ã‚‹ã“ã¨ã§ã™ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x7fdd786ee140 is calling an instrumented method <function Refine.get_response at 0x7fdef2970ee0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x7fdd884af550) using this function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã¯ã€è£½é€ æ‰€ã®åç§°ã‚„è£½é€ æ–¹æ³•ã€å“è³ªç®¡ç†è©¦é¨“ã€è¦æ ¼åŠã³è©¦é¨“æ–¹æ³•ã€å®‰å®šæ€§è©¦é¨“ãªã©ã®æƒ…å ±ã‚’ç™»éŒ²ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã¾ãŸã€è£½é€ å·¥ç¨‹ä¸­ã®ç´°èŒã€çœŸèŒã€ã‚¦ã‚¤ãƒ«ã‚¹ã®ä¸æ´»åŒ–ï¼é™¤å»å‡¦ç†æ–¹æ³•ã‚„è¦æ ¼åŠã³è©¦é¨“æ–¹æ³•ã«é–¢ã™ã‚‹æƒ…å ±ã‚‚å«ã¾ã‚Œã¾ã™ã€‚MFã‚’åˆ©ç”¨ã™ã‚‹éš›ã«ã¯ã€ç™»éŒ²è€…ã¨åˆ©ç”¨è€…é–“ã§æƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¤ã„ã¦ååˆ†ãªå”è­°ãŒå¿…è¦ã¨ã•ã‚Œã¦ã„ã¾ã™.\n"
     ]
    }
   ],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder_sentence_window as recording:\n",
    "        response = sentence_window_engine.query(question)\n",
    "        print(question)\n",
    "        print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "869d1e55-729b-45f2-a0f9-773c49d4616f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence Window Query Engine</th>\n",
       "      <td>0.325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.75</td>\n",
       "      <td>0.055591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Groundedness  Context Relevance  \\\n",
       "app_id                                                          \n",
       "Sentence Window Query Engine         0.325                1.0   \n",
       "\n",
       "                              Answer Relevance  latency  total_cost  \n",
       "app_id                                                               \n",
       "Sentence Window Query Engine               1.0    33.75    0.055591  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b92d0f2-2e80-48d5-92af-b3655eb03ea2",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path: https://s172-31-2-34p15913.lab-aws-production.deeplearning.ai/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33969 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33842 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33969 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33842 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33969 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33842 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33969 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 481, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 377, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 303, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 480, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 482, in <lambda>\n",
      "    lambda: self.imp(**ins)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 334, in relevance_with_cot_reasons\n",
      "    return self._extract_score_and_reasons_from_response(system_prompt)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 142, in _extract_score_and_reasons_from_response\n",
      "    response = self.endpoint.run_me(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 215, in run_me\n",
      "    raise RuntimeError(\n",
      "RuntimeError: API openai request failed 4 time(s).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 486, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of relevance_with_cot_reasons failed on inputs: \n",
      "{'prompt': 'MFã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚',\n",
      " 'response': ' \\n'\n",
      "             'äº‹å‹™é€£çµ¡ \\n'\n",
      "             'å¹³æˆï¼’ï¼•å¹´ï¼”æœˆ ï¼‘ï¼•æ—¥ \\n'\n",
      "             '\n",
      "API openai request failed 4 time(s)..\n",
      "\n",
      "openai request failed <class 'openai.BadRequestError'>=Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 33842 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 481, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 377, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 303, in track_all_costs\n",
      "    return Endpoint._track_costs(thunk, with_endpoints=endpoints)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 480, in _track_costs\n",
      "    result: T = thunk()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 482, in <lambda>\n",
      "    lambda: self.imp(**ins)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py\", line 151, in groundedness_measure_with_cot_reasons\n",
      "    reason = self.groundedness_provider._groundedness_doc_in_out(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 110, in _groundedness_doc_in_out\n",
      "    return self.endpoint.run_me(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 215, in run_me\n",
      "    raise RuntimeError(\n",
      "RuntimeError: API openai request failed 4 time(s).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 486, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of groundedness_measure_with_cot_reasons failed on inputs: \n",
      "{'source': ' \\n'\n",
      "           'äº‹å‹™é€£çµ¡ \\n'\n",
      "           'å¹³æˆï¼’ï¼•å¹´ï¼”æœˆ ï¼‘ï¼•æ—¥ \\n'\n",
      "           ' \\n'\n",
      "           'å„éƒ½é“åºœçœŒè¡›ç”Ÿä¸»ç®¡éƒ¨ï¼ˆå±€ï¼‰è–¬å‹™ä¸»ç®¡èª² å¾¡ä¸­ \\n'\n",
      "       \n",
      "API openai request failed 4 time(s)..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e2c55",
   "metadata": {},
   "source": [
    "### 2. Auto-merging retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "558c639b-31eb-4c34-b6c4-fe6ae5717733",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import build_automerging_index\n",
    "\n",
    "automerging_index = build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b32265f2-0247-42df-9abe-97d52f69edcf",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_automerging_query_engine\n",
    "\n",
    "automerging_query_engine = get_automerging_query_engine(\n",
    "    automerging_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "918ed568-220e-4c7c-aa60-cfa58ef1fcbd",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 89e4d1be-5eae-4f57-92aa-f042b7aff6d5.\n",
      "> Parent node text: 21http://www.pmda.go.jp/int -activities/int -harmony/ich/0035.htmlICHM4Qç¬¬ï¼’éƒ¨ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼’ï¼‰ï¼šå“è³ªã«é–¢ã™ã‚‹æ¦‚æ‹¬è³‡æ–™ã€ç¬¬\n",
      "...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: f6a451a8-fd4e-456c-9e98-8992ac433703.\n",
      "> Parent node text: 21http://www.pmda.go.jp/int -activities/int -harmony/ich/0035.htmlICHM4Qç¬¬ï¼’éƒ¨ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼’ï¼‰ï¼šå“è³ªã«é–¢ã™ã‚‹æ¦‚æ‹¬è³‡æ–™ã€ç¬¬\n",
      "...\n",
      "\n",
      "You can build a portfolio of AI projects by focusing on various aspects such as project structure, general characteristics, manufacturing details, manufacturers involved, manufacturing methods, process control, and related substances. It is important to follow the specified format and guidelines provided for creating the necessary documentation for each project.\n"
     ]
    }
   ],
   "source": [
    "auto_merging_response = automerging_query_engine.query(\n",
    "    \"How do I build a portfolio of AI projects?\"\n",
    ")\n",
    "print(str(auto_merging_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db4f18a9-7b8a-4ae2-ab11-3a6a941a5afc",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tru.reset_database()\n",
    "\n",
    "tru_recorder_automerging = get_prebuilt_trulens_recorder(automerging_query_engine,\n",
    "                                                         app_id=\"Automerging Query Engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99cc2cfe-7096-4fa0-aa72-094bebac35a3",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFã¨ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã¯ã€Œè–¬äº‹æ³•ã€ã§è¦å®šã•ã‚ŒãŸåŒ»è–¬å“ã®è£½é€ ã«ãŠã„ã¦ä½¿ç”¨ã•ã‚Œã‚‹åŸè–¬ã€ä¸­é–“ä½“ã€è£½å‰¤åŸæ–™ã®å“è³ªåŠã³å®‰å…¨æ€§ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ç™»éŒ²åˆ¶åº¦ã§ã™ã€‚\n",
      "MFã®ç”³è«‹æ›¸é¡ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã®ç”³è«‹æ›¸é¡ã¯ã€ç™»éŒ²ç”³è«‹æ›¸ã¨ç™»éŒ²ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\n",
      "USPã«ç™»éŒ²ãŒã‚‹åŸè–¬ã‚’MFã«ç™»éŒ²ã™ã‚‹éš›ã®æ³¨æ„ç‚¹ã¯ãªã‚“ã§ã™ã‹ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "åŸè–¬ã€ä¸­é–“ä½“åŠã³è£½å‰¤åŸæ–™ï¼ˆãƒãƒ«ã‚¯ã®ã†ã¡ç‰¹æ®Šãªå‰¤å‹ç­‰ï¼‰ã«ã¤ã„ã¦ã¯ã€å“è³ªåŠã³å®‰å…¨æ€§ãŒå¾“æ¥ã®è¦æ ¼åŠã³è©¦é¨“æ–¹æ³•ã«ãŠã„ã¦ã‚‚ç¢ºç«‹ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¨è€ƒãˆã‚‰ã‚Œã¦ãŠã‚Šã€å½“é¢ã€MFã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã¯å·®ã—æ§ãˆã‚‰ã‚ŒãŸã„ã€‚\n",
      "MFã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n",
      "MFã¯ã€æ—¥æœ¬è–¬å±€æ–¹ã«åè¼‰ã•ã‚ŒãŸåŒ»è–¬å“ã®æ‰¿èªå¯©æŸ»ã«ãŠã„ã¦ä½¿ç”¨ã•ã‚Œã‚‹ç™»éŒ²åˆ¶åº¦ã§ã™ã€‚äººã‚„å‹•ç‰©ã«å…±é€šã«ä½¿ç”¨ã•ã‚Œã‚‹åŒ»è–¬å“ã®å ´åˆã€MFã‚’åˆ©ç”¨ã™ã‚‹éš›ã¯ã€æ©Ÿæ§‹ã«MFç™»éŒ²ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚MFç™»éŒ²ç•ªå·ãŒä»˜ä¸ã•ã‚Œã€ãã®ç•ªå·ã‚’ç”¨ã„ã¦åŒ»è–¬å“ã‚„åŒ»ç™‚æ©Ÿå™¨ã®å–æ‰±ã„ã«é–¢ã™ã‚‹æƒ…å ±ãŒç®¡ç†ã•ã‚Œã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder_automerging as recording:\n",
    "        response = automerging_query_engine.query(question)\n",
    "        print(question)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5404dec1-60ca-42fa-ac13-793a5423aa64",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Automerging Query Engine</th>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Groundedness  Context Relevance  Answer Relevance  \\\n",
       "app_id                                                                        \n",
       "Automerging Query Engine      0.838889                0.6               1.0   \n",
       "\n",
       "                          latency  total_cost  \n",
       "app_id                                         \n",
       "Automerging Query Engine      4.0    0.000732  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f545d41-0d98-446f-8214-8b59bef08d6c",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path: https://s172-31-2-34p15913.lab-aws-production.deeplearning.ai/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
